# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.
"""
Scores a list of tiles specified at the top of this script.

Scoring is not batched i.e. batch size is 1. Some squeeze() need to be rid of to use for larger batch sizes.
"""

import argparse
import importlib
import json
import math
import os
import sys
from datetime import datetime

import numpy as np
import rasterio
import torch
from tqdm import tqdm

# SPECIFY input_tiles as a list of absolute paths to imagery .tif files

# with open('/home/boto/wcs/pycharm/constants/splits/full_sr_median_2013_2014_splits.json') as f:
#     input_tiles_json = json.load(f)
# val_tiles = input_tiles_json['val']
# input_tiles = [os.path.join('/boto_disk_0/wcs_data/tiles/full_sr_median_2013_2014/tiles', i) for i in
#                val_tiles]

# all tiles for the two-year composite
tiles_dir = '/home/boto/wcs/mnt/wcs-orinoquia/images_sr_median/2013_2014_dem'
input_tiles = [os.path.join(tiles_dir, i) for i in os.listdir(tiles_dir) if i.endswith('.tif')]


def write_colormap(tile_writer, config):
    """Write the experiment config's RasterLabelVisualizer's colormap to a TIFF file"""
    tile_writer.write_colormap(1, config.label_viz.get_tiff_colormap())  # 1 for band 1


def main():
    parser = argparse.ArgumentParser(description='Inference on tiles')
    parser.add_argument(
        '--config_module_path',
        required=True,
        help="Path to the .py file containing the experiment's configurations"
    )
    parser.add_argument(
        '--checkpoint_path',
        required=True,
        help='Path to the checkpoint .tar file to use for scoring'
    )
    parser.add_argument(
        '--out_dir',
        default='./outputs',
        help='Path to a dir to put the prediction tiles.'
    )
    parser.add_argument(
        '--batch_size',
        default=32,
        type=int,
        help='How many chips form one batch during scoring.'
    )
    parser.add_argument(
        '--output_softmax',
        action='store_true',
        help='Outputs the softmax probability scores as tiff tiles too, and its visualization as RGB tiles.'
    )
    args = parser.parse_args()

    assert os.path.exists(args.checkpoint_path), f'Checkpoint at {args.checkpoint_path} does not exist.'
    assert args.batch_size > 0, f'Invalid batch size: {args.batch_size}'

    out_dir = args.out_dir
    os.makedirs(out_dir, exist_ok=True)

    # config for the training_wcs run that generated the model checkpoint
    try:
        module_name = 'config'
        spec = importlib.util.spec_from_file_location(module_name, args.config_module_path)
        config = importlib.util.module_from_spec(spec)
        sys.modules[module_name] = config
        spec.loader.exec_module(config)
    except Exception as e:
        print(f'Failed to import the configurations. Exception: {e}')
        sys.exit(1)

    # device configuration
    print(f'Using PyTorch version {torch.__version__}.')
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'Using device: {device}.')

    # scoring configuration
    chip_size = config.chip_size
    prediction_window_size = config.prediction_window_size if config.prediction_window_size else 128
    prediction_window_offset = int((chip_size - prediction_window_size) / 2)
    print((f'Using chip_size {chip_size} and window_size {prediction_window_size}. '
           f'So window_offset is {prediction_window_offset}'))

    start_time = datetime.now()

    # instantiate the model and then load its state from the given checkpoint
    model = config.model

    checkpoint = torch.load(args.checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['state_dict'])
    print(f"Using checkpoint at epoch {checkpoint['epoch']}, step {checkpoint['step']}, "
          f"val accuracy is {checkpoint.get('val_acc', 'Not Available')}")

    model = model.to(device=device)

    # for each tile, form batches and score them; after scoring, write the output of each chip in the batch
    # to the output geotiff
    for i_file, input_tile_path in enumerate(input_tiles):

        out_path_hardmax = os.path.join(out_dir, 'res_' + os.path.basename(input_tile_path))
        if os.path.exists(out_path_hardmax):
            print(f'Skipping already scored tile {out_path_hardmax}')
            continue

        print(f'Scoring input tile {i_file} out of {len(input_tiles)}, {input_tile_path}')

        # dict_scores = {}  # dict of window tuple to numpy array of scores

        # load entire tile into memory
        tile_reader = rasterio.open(input_tile_path)

        # set up hardmax prediction output tile
        tile_writer_hardmax = rasterio.open(
            out_path_hardmax,
            'w',
            driver='GTiff',
            height=tile_reader.height,
            width=tile_reader.width,
            count=1,  # only 1 "band", the hardmax predicted label at the pixel
            dtype=np.uint8,
            crs=tile_reader.crs,
            transform=tile_reader.transform,
            nodata=0,
            compress='lzw',
            blockxsize=prediction_window_size,
            # reads and writes are most efficient when the windows match the datasetâ€™s own block structure
            blockysize=prediction_window_size
        )
        write_colormap(tile_writer_hardmax, config)

        # set up the softmax output tile
        if args.output_softmax:
            out_path_softmax = os.path.join(out_dir, 'prob_' + os.path.basename(input_tile_path))
            # probabilities projected into RGB for intuitive viewing
            out_path_softmax_viz = os.path.join(out_dir, 'prob_viz_' + os.path.basename(input_tile_path))

            tile_writer_softmax = rasterio.open(
                out_path_softmax,
                'w',
                driver='GTiff',
                height=tile_reader.height,
                width=tile_reader.width,
                count=config.num_classes,  # as many "bands" as there are classes to house the softmax probabilities
                # quantize probabilities scores so each can be stored as one byte instead of 4-byte float32
                dtype=np.uint8,
                crs=tile_reader.crs,
                transform=tile_reader.transform,
                nodata=0,
                compress='lzw',
                blockxsize=prediction_window_size,
                blockysize=prediction_window_size
            )
            tile_writer_softmax_viz = rasterio.open(
                out_path_softmax_viz,
                'w',
                driver='GTiff',
                height=tile_reader.height,
                width=tile_reader.width,
                count=3,  # RGB
                dtype=np.uint8,
                crs=tile_reader.crs,
                transform=tile_reader.transform,
                nodata=0,
                compress='lzw',
                blockxsize=prediction_window_size,
                blockysize=prediction_window_size
            )

        # use the get_chip function (to normalize the bands in a way that's consistent with training_wcs)
        # but get a chip that's the size of the tile - all at once
        whole_tile_window = (0, 0, tile_reader.width, tile_reader.height)
        data_array: np.ndarray = config.get_chip(tile_reader, whole_tile_window, chip_for_display=False)
        # TODO - could simplify to
        # data_array: np.ndarray = config.preprocess_tile(tile_reader)

        # pad by mirroring at the edges to facilitate predicting only on the center crop
        data_array = np.pad(data_array,
                            [
                                (0, 0),  # only pad height and width
                                (prediction_window_offset, prediction_window_offset),  # height / rows
                                (prediction_window_offset, prediction_window_offset)  # width / cols
                            ],
                            mode='symmetric')

        # form batches
        batch_size = args.batch_size
        batches = []
        batch_indices = []  # cache these to save recalculating when filling in model predictions

        # score the tile with overlapping sliding windows
        num_rows = math.ceil(tile_reader.height / prediction_window_size)
        num_cols = math.ceil(tile_reader.width / prediction_window_size)

        print('Forming batches...')
        for col_idx in tqdm(range(num_cols)):
            col_start = col_idx * prediction_window_size
            col_end = col_start + chip_size

            for row_idx in range(num_rows):
                row_start = row_idx * prediction_window_size
                row_end = row_start + chip_size

                chip = data_array[:, row_start:row_end, col_start: col_end]
                # pad to (chip_size, chip_size)
                chip = np.pad(chip,
                              [(0, 0), (0, chip_size - chip.shape[1]), (0, chip_size - chip.shape[2])])

                # processing it as the dataset loader _get_chip does
                chip = np.nan_to_num(chip, nan=0.0, posinf=1.0, neginf=-1.0)
                sat_mask = chip[0].squeeze() > 0.0  # mask out DEM data where there's no satellite data
                chip = chip * sat_mask

                batches.append(chip)

                valid_height = min(prediction_window_size, tile_reader.height - row_idx * prediction_window_size)
                valid_width = min(prediction_window_size, tile_reader.width - col_idx * prediction_window_size)
                batch_indices.append(
                    (row_start, valid_height, col_start, valid_width))  # explicit to be less confusing

        batches = np.array(batches)

        # score chips in batches and write the results to the output geotiff
        print('Scoring batches...')
        model.eval()
        with torch.no_grad():
            for i in tqdm(range(0, len(batches), batch_size)):
                t_batch = batches[i:i + batch_size]
                t_batch = torch.from_numpy(t_batch).to(device=device, dtype=config.dtype)

                try:
                    scores = model(t_batch)  # these are scores before the final softmax
                except Exception as e:
                    print(f'Exception in scoring loop model() application: {e}')
                    print(f't_batch has shape {t_batch.shape}')
                    sys.exit(1)

                _, preds = scores.max(1)
                preds = preds.cpu().numpy().astype(np.uint8)
                assert np.max(preds) < config.num_classes

                softmax_scores = torch.nn.functional.softmax(scores,
                                                             dim=1).cpu().numpy()  # (batch_size, num_classes, H, W)

                # write the output one chip at a time
                cur_batch_indices = batch_indices[i:i + batch_size]
                for chip_i, (row_start, valid_height, col_start, valid_width) in enumerate(cur_batch_indices):
                    # model output needs to be cropped to the window so they can be written correctly into the tile
                    # preds has a batch dim here; last dim is the inner most, x, width
                    preds_chip = preds[chip_i,
                             prediction_window_offset:prediction_window_offset + valid_height,
                             prediction_window_offset:prediction_window_offset + valid_width]
                    preds_chip = np.expand_dims(preds_chip, axis=0)
                    # debug - print(f'col is {col_idx}, row is {row_idx}, valid_window_tup is {valid_window_tup}, preds shape: {preds.shape}, preds1 shape: {preds1.shape}')

                    window = rasterio.windows.Window(col_start, row_start,
                                                     valid_width, valid_height)
                    # print(f'window is {window}')
                    tile_writer_hardmax.write(preds_chip, window=window)

                    if args.output_softmax:
                        # cropping, e.g. from (1, 14, 256, 256) to (1, 14, 128, 128)
                        softmax_scores = softmax_scores[:, :,
                                         prediction_window_offset:prediction_window_offset + valid_height,
                                         prediction_window_offset:prediction_window_offset + valid_width]

                        # get rid of batch dim. First dim for TIFF writer needs to be number of bands to write
                        softmax_scores = softmax_scores.squeeze()

                        # quantize using 256 buckets so probability value is writen in 1 byte instead of 4 (float32)
                        softmax_scores_quantized = (softmax_scores * 255).astype(np.uint8)

                        tile_writer_softmax.write(softmax_scores_quantized, window=window)

                        # softmax_scores_project is of dims ((batch_size), H, W, 3)
                        # make sure to not use the quantized version!
                        softmax_scores_proj = config.label_viz.visualize_softmax_predictions(softmax_scores)

                        softmax_scores_proj = softmax_scores_proj.squeeze()  # assume batch_size is 1 TODO

                        # we need it to be (3, H, W) to write to TIFF
                        softmax_scores_proj = np.transpose(softmax_scores_proj, axes=(2, 0, 1)).astype(np.uint8)
                        # debug - print(f'softmax_scores_proj min is {np.min(softmax_scores_proj)}, max is {np.max(softmax_scores_proj)}')

                        tile_writer_softmax_viz.write(softmax_scores_proj, window=window)

        tile_writer_hardmax.close()
        if args.output_softmax:
            tile_writer_softmax.close()
            tile_writer_softmax_viz.close()

    duration = datetime.now() - start_time
    print(f'Inference finished in {duration}, which is {duration / len(input_tiles)} per tile')


if __name__ == '__main__':
    main()
